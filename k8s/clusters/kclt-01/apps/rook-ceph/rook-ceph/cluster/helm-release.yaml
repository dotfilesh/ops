---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 15m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.12.5
      sourceRef:
        kind: HelmRepository
        name: rook-ceph
        namespace: flux-system
  maxHistory: 3
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 5
  values:
    operatorNamespace: rook-ceph
    monitoring:
      enabled: true
      createPrometheusRules: true
    configOverride: |
      [global]
      bdev_enable_discard = true
      bdev_async_discard = true
    ingress:
      dashboard:
        ingressClassName: "nginx"
        host:
          name: &host "rook.${CLUSTER_NAME}.sys.${GLOBALSECRET__PUBLIC_DOMAIN}"
          path: "/"
        tls:
          - hosts:
            - *host
            secretName: *host 
    cephClusterSpec:
      network:
        provider: host
      crashCollector:
        disable: false
      dashboard:
        enabled: true
        urlPrefix: /
        ssl: false 
      storage:
        # Set to false for safety, all nodes should be used though.
        useAllNodes: false
        useAllDevices: false
        nodes:
          #- name: formula
          #  devicePathFilter: ""
          #- name: urban
          - name: lailah
            devices:
              - name: "/dev/disk/by-id/wwn-0x55cd2e404c1ca321"
              - name: "/dev/disk/by-id/wwn-0x55cd2e414d948115"
              - name: "/dev/disk/by-id/wwn-0x5000c500dc5504c0" 
          - name: verus
            devices:
              - name: "/dev/disk/by-id/wwn-0x55cd2e404c1ca2fd"
              - name: "/dev/disk/by-id/wwn-0x55cd2e414d94853c"
              - name: "/dev/disk/by-id/wwn-0x50014ee6070ecea1"
          - name: mastema
            devices:
              - name: "/dev/disk/by-id/wwn-0x500000e01f8e20d0"
              - name: "/dev/disk/by-id/wwn-0x500003975850df09"
              - name: "/dev/disk/by-id/wwn-0x50000397585306fd"
              - name: "/dev/disk/by-id/wwn-0x50000396f8325511"
              - name: "/dev/disk/by-id/wwn-0x50000396f8325375"
              - name: "/dev/disk/by-id/wwn-0x50000396b80b1a15"
              - name: "/dev/disk/by-id/wwn-0x50000396f8329825"
              - name: "/dev/disk/by-id/wwn-0x50000396f8325515"
          #- name: amdusias
      resources:
        mgr:
          limits:
            memory: "1Gi"
          requests:
            cpu: "500m"
            memory: "512Mi"
        mon:
          limits:
            memory: "2Gi"
          requests:
            cpu: "1000m"
            memory: "1Gi"
        osd:
          limits:
            memory: "4Gi"
          requests:
            cpu: "1000m"
            memory: "4Gi"
        prepareosd:
          limits:
            memory: "2Gi"
          requests:
            cpu: "500m"
            memory: "50Mi"
        mgr-sidecar:
          limits:
            memory: "100Mi"
          requests:
            cpu: "100m"
            memory: "40Mi"
        crashcollector:
          limits:
            memory: "60Mi"
          requests:
            cpu: "100m"
            memory: "60Mi"
        logcollector:
          limits:
            memory: "1Gi"
          requests:
            cpu: "100m"
            memory: "100Mi"
        cleanup:
          limits:
            memory: "1Gi"
          requests:
            cpu: "500m"
            memory: "100Mi"
    cephBlockPools: 
      - name: ceph-blockpool
        spec:
          failureDomain: host
          replicated:
            size: 3
            hybridStorage:
              primaryDeviceClass: ssd
              secondaryDeviceClass: hdd
        storageClass:
            enabled: true
            name: ${CLUSTER_STORAGE_BLOCK}
            isDefault: true 
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            parameters:
              imageFormat: "2"
              imageFeatures: layering
              csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
              csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
              csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
              csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
              csi.storage.k8s.io/fstype: ext4
      - name: ceph-blockpool-hdd
        spec:
          failureDomain: host
          replicated:
            size: 3
            deviceClass: hdd
        storageClass:
            enabled: true
            name: ${CLUSTER_STORAGE_BLOCK_HDD}
            isDefault: false 
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            parameters:
              imageFormat: "2"
              imageFeatures: layering
              csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
              csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
              csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
              csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
              csi.storage.k8s.io/fstype: ext4
      - name: ceph-blockpool-ssd
        spec:
          failureDomain: host
          replicated:
            size: 3
            deviceClass: ssd
        storageClass:
            enabled: true
            name: ${CLUSTER_STORAGE_BLOCK_SSD}
            isDefault: false
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            parameters:
              imageFormat: "2"
              imageFeatures: layering
              csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
              csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
              csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
              csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
              csi.storage.k8s.io/fstype: ext4
    cephFileSystems:
      - name: ceph-filesystem
        spec:
          metadataPool:
            failureDomain: host
            deviceClass: ssd
            replicated:
              size: 3
          dataPools:
            - failureDomain: host
              replicated:
                size: 3
                hybridStorage:
                  primaryDeviceClass: ssd
                  secondaryDeviceClass: hdd
          metadataServer:
            activeCount: 1
            activeStandby: true
            resources:
              requests:
                cpu: "35m"
                memory: "64M"
              limits:
                memory: "144M"
        storageClass:
          enabled: true
          name: ${CLUSTER_STORAGE_FILESYSTEM}
          isDefault: false
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
      - name: ceph-filesystem-hdd
        spec:
          metadataPool:
            failureDomain: host
            deviceClass: ssd
            replicated:
              size: 3
          dataPools:
            - failureDomain: host
              deviceClass: hdd
              replicated:
                size: 3
          metadataServer:
            activeCount: 1
            activeStandby: true
            resources:
              requests:
                cpu: "35m"
                memory: "64M"
              limits:
                memory: "144M"
        storageClass:
          enabled: true
          name: ${CLUSTER_STORAGE_FILESYSTEM_HDD}
          isDefault: false
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
      - name: ceph-filesystem-ssd
        spec:
          metadataPool:
            failureDomain: host
            deviceClass: ssd
            replicated:
              size: 3
          dataPools:
            - failureDomain: host
              deviceClass: ssd
              replicated:
                size: 3
          metadataServer:
            activeCount: 1
            activeStandby: true
            resources:
              requests:
                cpu: "35m"
                memory: "64M"
              limits:
                memory: "144M"
        storageClass:
          enabled: true
          name: ${CLUSTER_STORAGE_FILESYSTEM_SSD}
          isDefault: false
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
    cephObjectStores:
      - name: ceph-bucket
        spec:
          metadataPool:
            failureDomain: host
            replicated:
              size: 3
          dataPool:
            failureDomain: host
            erasureCoded:
              dataChunks: 2
              codingChunks: 1
          gateway:
            port: 80
            instances: 1
            type: s3
            resources:
              requests:
                cpu: 500m
                memory: 1Gi
              limits:
                memory: 2Gi
          healthCheck:
            bucket:
              interval: 60s
        storageClass:
          enabled: true
          name: ${CLUSTER_STORAGE_BUCKET}
          reclaimPolicy: Delete
          parameters:
            region: ${CLUSTER_NAME}
    toolbox:
      enabled: true
